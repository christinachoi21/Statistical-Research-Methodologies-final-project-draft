---
title: 'Data Analysis Report – Draft 3: 36-290 Fall 2021'
author: "christina choi"
date: "10/22/2021"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{R, echo=FALSE}
rm(list=ls())
file.path = "https://github.com/pefreeman/36-290/raw/master/PROJECT_DATASETS/ACTIVE_CLASS/active_class.Rdata"
load(url(file.path))
rm(file.path)
```

## Classification of Active Galaxies Observed by SDSS

# Introduction

The Sloan Digital Sky Survey has a catalog containing data on over 200 million galaxies. Galaxy data typically include images along with measures of brightness from five different bandpasses (denoted u, g, r, i, and z) spanning the optical regime of the electromagnetic spectrum. These measures of brightness, or magnitudes, can reveal interesting information about galaxies. When a galaxy is *active*--meaning it forms stars at a relatively greater rate or has a supermassive black hole in its center that consumes stars/gas/dust at an enhanced rate--its spectrum will reveal "spikes" called emission lines. These emission lines along with other features from a spectra can be used to make inferences about whether the galaxy is star-forming or whether it has active nucleus.

We will attempt to classify galaxies as either starform or having an active nucleus using features from their spectra with the dataset created by Zhang et al. (2019).

----------------------------------------------------------------------------------------------------------------------------------
Variables                Description of Variables
       
------------------       ---------------------------------------------------------------------------------------------------------
**sigma_star**           The standard deviation of star velocities in the galaxy
                       
  
**sigma_o3**             The width of the [O III] line


**O3_Hb**                The relative strength of the [O III] emission line at 500.7 nanometers and the H beta line at                                      486.1 nm. O III refers to an emission line associated with oxygen atoms that are two                                               electrons short of a full set of electrons
  
  
**O2_Hb**                The relative strength of the [O II] emission line at 372.6 nanometers and the H beta line at                                       372.9 nm. O II refers to an emission line associated with oxygen atoms that are one electron                                       short of a full set of electrons
  
  
**u_g, g_r, r_i, i_z**   The four colors of the galaxy. The colors are differences in logarithmic measures of brightness, 
                         or magnitudes. Magnitudes are highly correlated with each other as well as galaxy distance.
 
 
**z**                    Galaxy redshift. Redshift refers to the ratio of the observed wavelength of a photon from an                                       object to its wavelength when it was emitted, minus 1.

  
**label**                The factor response variable in this dataset, that indicates whether a given galaxy is                                             observed to be star-forming (denoted "STARFORM") or with active nuclei (denoted "AGN").
-----------------------------------------------------------------------------------------------------------------------------------

Table: There are a total of nine predictor variables used in this investigation, with one response variable:

# Data

```{r echo=FALSE}
suppressMessages(library(magrittr))
suppressMessages(library(dplyr))
suppressMessages(library(ggplot2))
suppressMessages(library(GGally))
suppressMessages(library(corrplot))
suppressMessages(library(tidyr))
suppressMessages(library(bestglm))
suppressMessages(library(glmnet))
suppressMessages(library(tidyverse))
suppressMessages(library(pROC))
suppressMessages(library(e1071))
suppressMessages(library(rpart))
suppressMessages(library(randomForest))
suppressMessages(library(FNN))
suppressMessages(library(xgboost))
```


The variable `df` is a data frame with 28,820 rows and ten columns that contains information on 28820 different galaxies. The last column of the data frame named `label` contains the response variable, which labels each galaxy as either star-forming (denoted "STARFORM") or with active nuclei (denoted "AGN").

The response classes are not perfectly balanced, as there are 15,521 STARFORM and 13299 AGN observations. STARFORM galaxies make up roughly 53.9% of the dataset, while AGN galaxies make up roughly 46.1% of the data.

The following histograms visualize the distribution of the nine predictor variables mentioned above. 

There are several outliers present in the data. In order to get rid of the irregularities in the data, the rows for which g-r is exactly 0 have been filtered out. A similar process was used to correct the spikes in the data for sigma_star at 0 (which potentially indicates unmeasured values) and above 2.9. Below shows the distribution of the variables after the data has been filtered.

```{r}
df<- df %>% filter(., g_r!= 0, sigma_star!= 0, sigma_star<2.9)
```

```{r}
df.gathered <- df %>% select(.,u_g,g_r,r_i,i_z,O3_Hb,O2_Hb,sigma_o3,sigma_star,z) %>% gather(.)
ggplot(data=df.gathered,mapping=aes(x=value)) + geom_histogram(color="blue",fill="yellow",bins=60) + 
  facet_wrap(~key, scales='free')
```

After being filtered,`df` now has 21159 rows and ten columns. There are now 9873 STARFORM galaxies (which make up roughly 46.7% of the total data), and 11286 AGN galaxies (which make up roughly 53.3% of the total data).

Below is a summary of the dataset that we will be working with, and a list of the names of the nine predictor variables. Missing data has been filtered out, as mentioned above. 


```{r}
summary(df)
```
Looking at the boxplots that display the distribution of the different variables as well as their respective labels, there does seem to be a visual association between the predictors and the classes "AGN" and "STARFORM". 

For example, taking a look at the color predictor variables, the galaxies labeled as AGN tend to have greater median magnitudes compared to those labeled as STARFORM. 


```{r}
df.facet = df %>% dplyr::select(.,-label) %>% gather(.)
ggplot(data=df.facet,mapping=aes(y=value,x=rep(df$label,9))) + geom_boxplot(fill="lightblue") + facet_wrap(~key,scales='free_y')
```

Next, we want to look at the correlation between some of the variables.

```{r}
df %>% dplyr::select(.,u_g, g_r,r_i, i_z,O3_Hb,O2_Hb,sigma_o3,sigma_star,z) %>% cor(.) %>% corrplot(.,method="number")
```
It seems like most of the color variables are moderately if not strongly correlated to each other. O2_Hb and O3_Hb have fair correlation, along with Sigma_O3 and O3_Hb. Generally, the correlation coefficients seem to be at least above 0.3, indicating some level of correlation between most if not all the variables.


# Principal Component Analysis

After we have filtered out the missing values, looked at the correlation between the predictors, and visualized the different distributions of the variables, we will be using Principal Component Analysis, or PCA. 

In this case, since we are interested in prediction (not inference) we will be mainly using PCA to see if the data lay in a lower-dimensional space.

We now perform principal components analysis using a dataset with only the predictor variables. We also make sure to scale the variables so that they are standardized.

```{r}
df.no.lab = subset(df, select = -label)
pca.out = prcomp(df.no.lab, scale=TRUE, retx = TRUE, center = TRUE, tol = NULL)

v = pca.out$sdev^2
pve = v/sum(v)
cpve = round(cumsum(v/sum(v)),3)
cpve


pr_var = data.frame(varExplained = pca.out$sdev^2)
pr_var = pr_var %>% mutate(pve = varExplained / sum(varExplained))
```

We can see that the first principal component explains 38.1% of the variance in the data, the next PC explains 58.1% of the variance, and so on and so forth. We plot the cumulative PVE as follows:

```{r}
ggplot(pr_var, aes(as.numeric(row.names(pr_var)), cumsum(pve))) +
  geom_point() + geom_line()+ geom_hline(yintercept=.95, linetype="dashed", color="grey", size=.6) +
  xlab("Principal Component") +
  ylab("Cumulative Proportion of Variance Explained")
```

Looking at the plot of the cumulative proportion of variance explained,it seems that around 7 PCs should be retained. If we were to adopt 7 PCs, we would be able to drop the last two principal components, which we can assume represents the random variation in the data, and still be able to reconstruct the input data with 97.4% of the overall variance "explained". It appears overall, PCA would provide some level of reduced dimensionality, but not too much.



# Logistic Regression

Now we move onto logistic regression, using a 70-30 data split for the training and testing sets.

```{r}

predictors = df.no.lab
response   = df$label

#split data, logistic regression model learned on split data with test-set assessment
set.seed(100)
fraction=.7
sp = sample(nrow(predictors), round(fraction*nrow(predictors)))
pred.train = predictors[sp ,]
pred.test = predictors[-sp ,]
resp.train = response[sp]
resp.test = response[-sp]


contrasts(df$label)

#logistic regression model using training data
glm.fit = glm(resp.train~.,data=pred.train,family="binomial")
summary(glm.fit)


glm.prob=predict(glm.fit,newdata=pred.test,type="response")

glm.roc = roc(resp.test,glm.prob)

plot(glm.roc,col="red",xlim=c(1,0),ylim=c(0,1))

cat("AUC for logistic regression: ",glm.roc$auc,"\n")

```

Below we calculate the VIF values of each variable in the model to assess whether or not there is any multicollinearity. It appears that g_r and r_i in particular have VIF values greater than 5, and it seems that there is some presence of multicollinearity but not too much. This is consistent with the results from PCA, where some level of dimensionality could be dropped but not by a lot. Since our objective is prediction, we are not removing variables with high VIF.


```{r}
car::vif(glm.fit)
```

# Best Subset Selection 
```{r, warning=FALSE}
df.train = cbind(pred.train,resp.train)
df.test = cbind(pred.test,resp.test)


names(df.train)[names(df.train) == "resp.train"] <- "y"
names(df.test)[names(df.test) == "resp.test"] <- "y"


bg.outBIC = bestglm(df.train,family=binomial, IC="BIC")

bg.outAIC= bestglm(df.train,family=binomial, IC="AIC")

```

```{r}
bg.outBIC$BestModel

bg.outAIC$BestModel
``` 

For the BIC model, we retain five of the nine predictor variables, dropping variables z, u_g, r_i, i_z.
For the AIC model, we retain six of the nine predictor variables, dropping i_z, r_i, and g_r

```{r}
AIC.resp.prob = predict(bg.outAIC$BestModel, newdata=df.test, type="response")

AIC.resp.pred = ifelse(AIC.resp.prob>0.5,"AGN", "STARFORM")


mcr.AIC = mean(AIC.resp.pred!=resp.test)
mcr.AIC

mean(AIC.resp.pred==resp.test)

table(AIC.resp.pred, resp.test)

```
AIC misclassification rate is 0.006931317.


# Lasso and Ridge Regression

We attempted to see if lasso and ridge regression was able to be used in this case. 

```{r}
#lasso
x = model.matrix(resp.train~.,pred.train)[,-1] ; y = resp.train
out.lasso = glmnet(x,y,alpha=1,family = "binomial") 

set.seed(100)
cv.lasso = cv.glmnet(x,y,alpha=1,family = "binomial")

cv.lasso$lambda.min ; log(cv.lasso$lambda.min)
#5.132038e-05, -9.877423

coef(out.lasso, cv.lasso$lambda.min)

```
For lasso regression, given optimal value of lambda as 5.132038e-05, all nine variables were retained, suggesting that lasso regression did not lead to improved results.


```{r}
#ridge regresssion
out.ridge = glmnet(x,y,alpha=0,family = "binomial") 

set.seed(100)
cv.ridge = cv.glmnet(x,y,alpha=0,family = "binomial")

cv.ridge$lambda.min ; log(cv.ridge$lambda.min)
#0.04260708, -3.155735

coef(out.ridge, cv.ridge$lambda.min)
```

For ridge regression, given optimal value of lambda as 0.04260708, all nine variables were retained, suggesting that ridge regression, similar to lasso regression, did not lead to improved results.

This may have been due to the sample sizes being relatively small compared to the number of predictor variables in our data. 


# Machine Learning Models and KNN

Next, we will be working with various machine learning models (trees, RF, XGB, Naive Bayes) as well as KNN. Given the size of the dataset and the time necessary to run SVM, Support Vector Machine is not included in this section.

## Trees
```{r}
rpart.out =rpart(resp.train~.,data=pred.train)
tree.class.prob= predict(rpart.out,newdata=pred.test, type="prob")[,2] 
roc.tree = roc(resp.test,tree.class.prob)
plot(roc.tree,col="red",xlim=c(1,0),ylim=c(0,1))
cat("AUC for tree: ",roc.tree$auc,"\n")
```

## Random Forest

```{r}
set.seed(100)
rf.out = randomForest(resp.train~.,data=pred.train,importance=TRUE)
resp.prob.rf = predict(rf.out,newdata=pred.test,type="prob")[,2]
roc.rf = roc(resp.test,resp.prob.rf)
plot(roc.rf,col="red",xlim=c(1,0),ylim=c(0,1))
cat("AUC for random forest: ",roc.rf$auc,"\n")

```


## XGB

Since `xgboost` wants integer class labels, we map "STARFORM" to 0 and "AGN" to 1.
```{r}
w = which(response!="STARFORM")
response.new = rep(0,length(response))
response.new[w] = 1
response.int = response.new

set.seed(100)
fraction=.7
sp = sample(nrow(predictors), round(fraction*nrow(predictors)))
pred.train = predictors[sp ,]
pred.test = predictors[-sp ,]
resp.train.int = response.int[sp]
resp.test.int = response.int[-sp]


train = xgb.DMatrix(data=as.matrix(pred.train),label=resp.train.int)
test = xgb.DMatrix(data=as.matrix(pred.test),label=resp.test.int)

set.seed(101)
xgb.cv.out = xgb.cv(params=list(objective="binary:logistic"),train,nrounds=30,nfold=5,verbose=0,eval_metric="error") 
cat("The optimal number of trees is", which.min(xgb.cv.out$evaluation_log$test_error_mean))

xgb.out = xgboost(train,nrounds=which.min(xgb.cv.out$evaluation_log$test_error_mean),params=list(objective="binary:logistic"),verbose=0,eval_metric="error")

resp.predxg = predict(xgb.out,newdata=test,probability=TRUE)

roc.xgb = roc(resp.test,resp.predxg)


plot(roc.xgb,col="red",xlim=c(1,0),ylim=c(0,1))
cat("AUC for xgb: ",roc.xgb$auc,"\n")

```


## Naive Bayes
```{r}
nb.out = naiveBayes(resp.train~.,data=pred.train)
nb.prob = predict(nb.out,newdata=pred.test,type="raw")[,2]
roc.nb = roc(resp.test,nb.prob)
plot(roc.nb,col="red",xlim=c(1,0),ylim=c(0,1))
cat("AUC for naive bayes: ",roc.nb$auc,"\n")

```

## KNN
```{r}

k.max = 20
mse.k = rep(NA,k.max) 
for ( kk in 1:k.max ) {
  knn.out = knn.cv(train=pred.train,cl=resp.train,k=kk,algorithm="brute")
  mse.k[kk] = mean(knn.out != resp.train)
}
k.min = which.min(mse.k)
cat("The optimal number of nearest neighbors is ",k.min,"\n")

knn.out = knn(train=pred.train,test=pred.test,cl=resp.train,k=k.min,prob=TRUE, algorithm="brute")
knn.prob = attributes(knn.out)$prob
w = which(knn.out=="STARFORM") 
knn.prob[w] = 1 - knn.prob[w] 

roc.knn = roc(resp.test,knn.prob)
plot(roc.knn,col="red",xlim=c(1,0),ylim=c(0,1))
cat("AUC for knn: ",roc.knn$auc,"\n")

```


## Conclusion 

----------------------------------------------------------------------------------------------------------------------------------
Model                     AUC and MCR values
       
------------------       ---------------------------------------------------------------------------------------------------------
**Decision Tree**         AUC: 0.9847113 

**Random Forest**         AUC: 0.9999623  

**XGBoost**               AUC: 0.99997 

**Naive Bayes**           AUC: 0.9934819 

**KNN**                   AUC: 0.9941899 

**logistic regression**   AUC: 0.9998205 

**AIC**                   MCR: 0.006931317
-----------------------------------------------------------------------------------------------------------------------------------

It appears that XGboost has the greatest AUC value of 0.99997.

Now we use Youden's J to find the optimal threshold, make class predictions based on this threshold, and determine the MCR and confusion matrix for our optimal model using gradient boosting.

```{r}
J = roc.xgb$sensitivities + roc.xgb$specificities - 1

w = which.max(J)
cat("Optimum threshold for xgb: ",roc.xgb$thresholds[w],"\n")


resp.predxg = predict(xgb.out,newdata=test,probability=TRUE)
resp.pred.xg = ifelse(resp.predxg>0.3353565, "AGN","STARFORM")

table(resp.pred.xg,resp.test)
mean(resp.pred.xg!=resp.test)

plot(roc.xgb,col="red",xlim=c(1,0),ylim=c(0,1))
cat("AUC for xgb: ",roc.xgb$auc,"\n")



```

The optimum threshold for xgb is 0.3353565 and the MCR is 0.002678009.

Using xgboost which we determined had the highest AUC out of all the other models that were tested, we were able to classify galaxies as either starform or having an active nucleus with a misclassification rate of .2678009%. Looking at the confusion matrix above, it appears that of the 6348 galaxies within our test set, we were able to correctly classify 2952 galaxies as starform and 3379 as having active nucleus.


# References 

```

Freeman, P.E. 2021, online at https://github.com/pefreeman/36-290/blob/master/PROJECT_DATASETS/ACTIVE_CLASS/README.md 


James, G., et al. 2013, An Introduction to Statistical Learning, Springer


Zhang, K., et al. 2019, "Machine Learning Classifiers for Intermediate Redshift Emission Line Galaxies", The Astrophysical Journal, online at arxiv.org/pdf/1908.07046.pdf


Grolemund, Hadley Wickham and Garrett. “7 Exploratory Data Analysis | R for Data Science”, online at https://r4ds.had.co.nz/exploratory-data-analysis.html


```



